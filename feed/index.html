<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Julián Tachella</title>
	<atom:link href="https://tachella.github.io//feed/" rel="self" type="application/rss+xml" />
	<link>http://tachella.github.io//</link>
	<description></description>
	<lastBuildDate>Thu, 29 Jul 2021 16:57:27 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.8.2</generator>

<image>
	<url>https://tachella.github.io//wp-content/uploads/2019/10/cropped-me-1-150x150.jpg</url>
	<title>Julián Tachella</title>
	<link>http://tachella.github.io//</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Equivariant imaging: Learning beyond the range space</title>
		<link>https://tachella.github.io//2021/04/16/equivariant-imaging-learning-beyond-the-range-space/</link>
		
		<dc:creator><![CDATA[Julián]]></dc:creator>
		<pubDate>Fri, 16 Apr 2021 12:38:37 +0000</pubDate>
				<category><![CDATA[neural networks]]></category>
		<guid isPermaLink="false">https://tachella.github.io//?p=552</guid>

					<description><![CDATA[Authors: D. Chen, J. Tachella and M. Davies Conference: ICCV 2021 (Oral paper) Preprint: https://arxiv.org/abs/2103.14756 Online codes: https://github.com/edongdongchen/EI Inverse problems are ubiquitous in signal and image processing. In most applications, we need to reconstruct an underlying signal $x\in\mathbb{R}^{n}$, from some measurements $y\in\mathbb{R}^{m}$, that is, invert the forward measurement process, $$y = Ax+n$$ where $n$ represents&#46;&#46;&#46;]]></description>
										<content:encoded><![CDATA[
<p><strong>Authors:</strong> D. Chen, J. Tachella and M. Davies</p>



<p><strong>Conference: </strong>ICCV 2021 (Oral paper)</p>



<p><strong>Preprint: </strong><a href="https://arxiv.org/abs/2103.14756 ">https://</a><a href="https://arxiv.org/abs/2103.14756" data-type="URL" data-id="https://arxiv.org/abs/2103.14756">arxiv</a><a href="https://arxiv.org/abs/2103.14756 ">.org/abs/2103.14756</a></p>



<p><strong>Online codes: </strong><a href="https://github.com/edongdongchen/EI ">https://github.com/</a><a href="https://github.com/edongdongchen/EI">edongdongchen</a><a href="https://github.com/edongdongchen/EI ">/EI</a></p>



<p>Inverse problems are ubiquitous in signal and image processing. In most applications, we need to reconstruct an underlying signal $x\in\mathbb{R}^{n}$, from some measurements $y\in\mathbb{R}^{m}$, that is, invert the forward measurement process, $$y = Ax+n$$ where $n$ represents some noise and $A$ is the forward operator. Due to the ill-posed nature of $A$ (we generally have $m&lt;n$) and noise, there are multiple possible solutions $x$ for a given $y$. Fortunately, the set of plausible (natural) signals $x$ lie in a small low-dimensional set $\mathcal{X}$ of the whole of $\mathbb{R}^{n}$, so we can have a unique $x$ for a given $y$.</p>



<p>The traditional approach is to build a mathematical model to describe $\mathcal{X}$ leveraging some prior knowledge about the underlying signals (e.g. natural images can be described as piecewise smooth). However, this a hard task which is problem-dependent and it is generally a loose description of the true $\mathcal{X}$.</p>



<p>In recent years, an alternative approach is to learn inverse mapping from $y\mapsto x$ directly from training data, bypassing the need to design a prior model. Fuelled by the powerful learning bias of deep convolutional neural networks (interest readers can have a look at my <a href="https://tachella.github.io//2020/09/01/the-neural-tangent-denoiser/" data-type="post" data-id="344">previous post</a> about understanding this implicit bias), the goal is to learn a function $x=f(y)$ from training pairs $(x_i,y_i)$. The fundamental limitation of this approach is that in many real world applications we can only access $y$. Training only with the $y_i$ (enforcing measurement consistency) accounts to finding an $f$ such that $y=A f(y)$. Unfortunately this is doomed to fail, as there are infinite possible functions $f$ that can fit the measurements perfectly well! This is because any $f$ can output any value in the nullspace of $A$ and still achieve measurement consistency. In other words, this fundamental limitation is a chicken-and-egg problem:  we cannot learn to solve an inverse problem without solving it first to obtain the ground-truth training data!</p>



<p>In this paper, we show that this problem can be overcome by adding a small assumption to the underlying set of signals $\mathcal{X}$: invariance. It is well-known that most natural signals posses some kind of invariance. For example, images are generally invariant to shifts or rotations. Hence, the whole sensing process $x = (f \circ A) (x)$ is necessarily an equivariant function, that is, given a transformation $T_g$ (e.g. a shift), we have that $$T_gx = (f\circ A) (T_gx)$$. The invariance gives us information of the nullspace of A, which boils down to the following observation: $$y=Ax = AT_g x&#8217;  = A_g x&#8217;$$ which just relies on the fact that $x&#8217;= T_gx$ is another valid signal. Hence we can see beyond the range space of $A$, as we have an implicit access to multiple different operators  $A_g = AT_g$ for all possible transformations $T_1,\dots,T_{G}$.  </p>



<p>We show that this invariance constraint on $(f\circ A)$ can be easily incorporated as an additional loss term when training a deep network. Our experiments show that for the computed tomography and inpaiting problems,  the equivariant learning approach (only having access to measurements $y_i$) performs as well as the fully supervised case i.e. having training pairs with ground-truth data $(x_i,y_i)$, by-passing the fundamental limitation of learning to solve inverse problems. Check the paper for more details!</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="856" height="358" src="https://tachella.github.io//wp-content/uploads/2021/04/post_invariant-1.png" alt="" class="wp-image-630" srcset="https://tachella.github.io//wp-content/uploads/2021/04/post_invariant-1.png 856w, https://tachella.github.io//wp-content/uploads/2021/04/post_invariant-1-300x125.png 300w, https://tachella.github.io//wp-content/uploads/2021/04/post_invariant-1-768x321.png 768w, https://tachella.github.io//wp-content/uploads/2021/04/post_invariant-1-520x217.png 520w" sizes="(max-width: 856px) 100vw, 856px" /><figcaption>The proposed equivariant imaging learning framework can learn the reconstruction function only using compressed samples $y_i$ and still perform as well as a fully trained network which requires ground-truth pairs $(x_i,y_i)$. On the contrary, only enforcing measurement consistency is not enough to learn the reconstruction function.</figcaption></figure></div>



<p></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>A sketching framework for reduced data transfer in photon counting lidar</title>
		<link>https://tachella.github.io//2021/02/20/a-sketching-framework-for-reduced-data-transfer-in-photon-counting-lidar/</link>
		
		<dc:creator><![CDATA[Julián]]></dc:creator>
		<pubDate>Sat, 20 Feb 2021 18:50:39 +0000</pubDate>
				<category><![CDATA[Lidar]]></category>
		<guid isPermaLink="false">https://tachella.github.io//?p=435</guid>

					<description><![CDATA[Authors:&#160;M. P. Sheehan, J. Tachella and M. Davies Preprint:&#160;https://arxiv.org/abs/2102.08732 Additional sketched detection paper: &#8220;Surface Detection for Sketched Single Photon Lidar&#8221;, EUSIPCO 2021 (preprint available at https://arxiv.org/abs/2105.06920) Online codes:&#160;https://gitlab.com/Tachella/sketched_lidar Patent: https://edinburgh-innovations.ed.ac.uk/project/reduced-data-transfer-framework-for-single-photon-lidar/ Single-photon lidar is an emerging ranging technique that can obtain 3D information at kilometre distance with centimetre precision, and has important applications in self-driving cars,&#46;&#46;&#46;]]></description>
										<content:encoded><![CDATA[
<p><strong>Authors:</strong>&nbsp;M. P. Sheehan, J. Tachella and M. Davies</p>



<p><strong>Preprint:&nbsp;</strong><a href="https://www.nature.com/articles/s41467-020-19727-4"></a><a href="https://arxiv.org/abs/2102.08732">https://arxiv.org/abs/2102.08732</a></p>



<p><strong>Additional sketched detection paper: </strong>&#8220;Surface Detection for Sketched Single Photon Lidar&#8221;, EUSIPCO 2021 (preprint available at <a href="https://arxiv.org/abs/2105.06920">https://arxiv.org/abs/2105.06920</a>)</p>



<p><strong>Online codes:&nbsp;</strong><a href="https://gitlab.com/Tachella/sketched_lidar">https://gitlab.com/Tachella/sketched_lidar</a></p>



<p><strong>Patent: </strong><a href="https://edinburgh-innovations.ed.ac.uk/project/reduced-data-transfer-framework-for-single-photon-lidar/">https://edinburgh-innovations.ed.ac.uk/project/reduced-data-transfer-framework-for-single-photon-lidar/</a></p>



<p>Single-photon lidar is an emerging ranging technique that can obtain 3D information at kilometre distance with centimetre precision, and has important applications in <strong><a href="https://tachella.github.io//2020/08/01/advances-in-single-photon-lidar-for-autonomous-vehicles/" data-type="post" data-id="337">self-driving cars</a>, forest canopy monitoring, <a href="https://tachella.github.io//2020/09/01/seeing-around-corners-with-edge-resolved-transient-imaging/" data-type="post" data-id="429">non-line-of-sight imaging</a> </strong>and more. This modality consists of contracting a histogram of time-of-arrival of individual photons per pixel. For each object in the line-of-sight of the device there is a peak in the histogram. These peaks are found by a 3D reconstruction algorithm that takes into account the Poisson statistics of the photon-count data, while promoting spatial smoothness in the reconstructed point clouds. In a previous post, I presented <a href="https://tachella.github.io//2019/10/29/real-time-3d-reconstruction-from-single-photon-lidar-data-using-plug-and-play-point-cloud-denoisers/" data-type="post" data-id="210">an algorithm</a> that can find multiple peaks per pixel in a matter of milliseconds even in challenging very long range scenarios with high background noise. As the algorithm needs to process the histogram data, the <strong>reconstruction time</strong> depends (linearly) on the total number of non-zero bins in the histogram:</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img loading="lazy" src="https://tachella.github.io//wp-content/uploads/2021/02/timing_bins-1.png" alt="" class="wp-image-477" width="509" height="241"/><figcaption>Fig. 1: Execution time of a 3D reconstruction algorithm as a function the number of non-zero bins in the collected time-of-arrival histograms (from <a href="https://www.nature.com/articles/s41467-019-12943-7">this paper</a>).</figcaption></figure></div>



<p>As single-photon lidar arrays get bigger and faster, the number of photons collected per histogram gets bigger, while there is an increased need for faster real-time frame rates. The volume of photon data that needs to be transmitted is ever-increasingly large, generating a <strong>data transfer bottleneck</strong>. Moreover, reconstruction algorithms are required to deal with ever-increasingly large and dense histograms, generating a <strong>computational bottleneck</strong>. So far, most attempts to alleviate these bottlenecks consisted in building coarser histograms. Despite reducing the amount of information to be transferred and processed, this approach sacrifices important depth resolution.</p>



<p>Here, we propose a sketching method to massively <strong>compress the histograms without any significant loss of information</strong>, removing the data and computational bottlenecks. The technique builds on recent advances in <a href="https://arxiv.org/abs/1706.07180">compressive learning</a>, a theory for compressing distributions. The compressed data consists of a series of $K$ statistics </p>



<p>$$\Phi_k(t) = [\cos(w_k t),  \sin(w_kt)]^{T} \quad \text{for} \quad k=1, \dots, K$$</p>



<p>where $t$ denotes the time of arrival. The statistics can be <strong>computed on-the-fly</strong>, i.e. updated with each photon arrival, hence completely by-passing the need to construct a histogram. Below you can see the large difference between reducing the data by coarse binning the histogram and our proposed method:</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="1024" height="307" src="https://tachella.github.io//wp-content/uploads/2021/02/sketched_mini2.gif" alt="" class="wp-image-544"/></figure></div>



<p>Please check the paper for more information about the sketching technique!</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Seeing around corners with edge-resolved transient imaging</title>
		<link>https://tachella.github.io//2020/09/01/seeing-around-corners-with-edge-resolved-transient-imaging/</link>
		
		<dc:creator><![CDATA[Julián]]></dc:creator>
		<pubDate>Tue, 01 Sep 2020 18:36:00 +0000</pubDate>
				<category><![CDATA[Lidar]]></category>
		<category><![CDATA[Low-photon imaging]]></category>
		<guid isPermaLink="false">https://tachella.github.io//?p=429</guid>

					<description><![CDATA[Authors:&#160;J. Rapp*, C. Saunders*, J. Tachella*, J. Murray-Bruce, Y. Altmann, J-Y. Tourneret, S. McLaughlin, R. Dawson, F. Wong and V. K. Goyal. *Equal contribution Full paper:&#160;https://www.nature.com/articles/s41467-020-19727-4 Journal:&#160;Nature Communications Online codes:&#160;https://github.com/Computational-Periscopy/ERTI/ Seeing around the corners is a task that may sound&#160;impossible at first glance. This task has many important applications, such as&#160;anticipatory imaging in self-driving cars,&#46;&#46;&#46;]]></description>
										<content:encoded><![CDATA[
<p><strong>Authors:</strong>&nbsp;J. Rapp*, C. Saunders*, J. Tachella*, J. Murray-Bruce, Y. Altmann, J-Y. Tourneret, S. McLaughlin, R. Dawson, F. Wong and V. K. Goyal. *Equal contribution</p>



<p><strong>Full paper:&nbsp;</strong><a href="https://www.nature.com/articles/s41467-020-19727-4">https://www.nature.com/articles/s41467-020-19727-4</a></p>



<p><strong>Journal:&nbsp;</strong>Nature Communications</p>



<p><strong>Online codes:&nbsp;</strong><a href="https://github.com/Computational-Periscopy/ERTI/">https://github.com/Computational-Periscopy/ERTI/</a></p>



<p>Seeing around the corners is a task that may sound&nbsp;<strong>impossible at first glance</strong>. This task has many important applications, such as&nbsp;<strong>anticipatory imaging in self-driving cars, planetary exploration or search and rescue operations</strong>. In this paper, we present a novel imaging technique which allows to obtain 2.5 dimensional information about hidden (non-line-of-sight) scenes. Our method obtains angular information using corners and depth information using a time-resolved camera (a single-photon lidar). Recovering the hidden room configuration constitutes&nbsp;<strong>non-linear inverse problem</strong>, which we solve by proposing a careful modelling of the measurement operator (the light transport equation) and a reconstruction algorithm which exploits our prior knowledge about plausible configurations.</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="video-container"><iframe loading="lazy" title="Seeing Around Corners with Edge-Resolved Transient Imaging" width="740" height="416" src="https://www.youtube.com/embed/1MDwFVky-wg?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
</div></figure>



<p>Check the paper for more details!</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>The neural tangent link between CNN denoisers and non-local filters</title>
		<link>https://tachella.github.io//2020/09/01/the-neural-tangent-denoiser/</link>
		
		<dc:creator><![CDATA[Julián]]></dc:creator>
		<pubDate>Tue, 01 Sep 2020 13:31:04 +0000</pubDate>
				<category><![CDATA[neural networks]]></category>
		<guid isPermaLink="false">https://tachella.github.io//?p=344</guid>

					<description><![CDATA[Authors: J. Tachella, J. Tang and M. Davies Conference: CVPR 2021 (Oral paper)  Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tachella_The_Neural_Tangent_Link_Between_CNN_Denoisers_and_Non-Local_Filters_CVPR_2021_paper.html Online codes: https://gitlab.com/Tachella/neural_tangent_denoiser Convolutional neural networks (CNNs) are a well-established tool for solving computational imaging problems.  It has been recently shown that, despite being highly overparameterized (more weights than pixels), networks trained with a single corrupted image can still perform as&#46;&#46;&#46;]]></description>
										<content:encoded><![CDATA[<p><strong>Authors:</strong> J. Tachella, J. Tang and M. Davies</p>
<p><strong>Conference: </strong>CVPR 2021 (Oral paper) </p>
<p><strong>Paper: </strong><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Tachella_The_Neural_Tangent_Link_Between_CNN_Denoisers_and_Non-Local_Filters_CVPR_2021_paper.html">https://openaccess.thecvf.com/content/CVPR2021/html/Tachella_The_Neural_Tangent_Link_Between_CNN_Denoisers_and_Non-Local_Filters_CVPR_2021_paper.html</a></p>
<p><strong>Online codes: </strong><a href="https://gitlab.com/Tachella/neural_tangent_denoiser">https://gitlab.com/Tachella/neural_tangent_denoiser</a></p>
<p><div class="video-container"><iframe loading="lazy" title="The neural tangent link between CNN denoisers and non-local filters (CVPR 2021)" width="740" height="416" src="https://www.youtube.com/embed/vLxzxp2boyY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></p>
<p>Convolutional neural networks (CNNs) are a well-established tool for solving computational imaging problems.  It has been recently shown that, despite being highly overparameterized (more weights than pixels), networks trained with a single corrupted image can still perform as well as fully trained networks (<em>a.k.a. <a href="https://dmitryulyanov.github.io/deep_image_prior">the deep image prior</a></em>).  These results highlight that <strong>CNNs posses a very powerful learning bias towards natural images</strong>, which explains their great success in recent years. Multiple intriguing question arise:</p>
<ol>
<li>What is the learning bias?</li>
<li>Are neural networks performing something similar to other existing tools in signal processing?</li>
<li>Is the existing theory able to explain this phenomenon?</li>
</ol>
<p>In this paper, we make a first step towards answering these questions, using recent theoretical insights of infinitely wide networks (<em>a.k.a. <a href="https://arxiv.org/abs/1806.07572">the neural tangent kernel</a></em>), elucidating formal links between CNNs and well-known non-local patch denoisers, such as non-local means.</p>
<p><img loading="lazy" class="size-large wp-image-349 aligncenter" src="https://tachella.github.io//wp-content/uploads/2020/09/non_local-1024x200.png" alt="" width="740" height="145" srcset="https://tachella.github.io//wp-content/uploads/2020/09/non_local-1024x200.png 1024w, https://tachella.github.io//wp-content/uploads/2020/09/non_local-300x59.png 300w, https://tachella.github.io//wp-content/uploads/2020/09/non_local-768x150.png 768w, https://tachella.github.io//wp-content/uploads/2020/09/non_local-1536x300.png 1536w, https://tachella.github.io//wp-content/uploads/2020/09/non_local-520x102.png 520w, https://tachella.github.io//wp-content/uploads/2020/09/non_local-940x183.png 940w, https://tachella.github.io//wp-content/uploads/2020/09/non_local.png 1793w" sizes="(max-width: 740px) 100vw, 740px" /></p>
<p>Non-local means uses the following non-local similarity function:</p>
<p>$$ k(y_i, y_j) = \exp(-||y_i-y_j||^2/\sigma^2)$$<br />where $y_i$ and $y_j$ are small image patches (e.g. $5\times 5$ pixels) around the pixels $i$ and $j$. The filter matrix $W$ is constructed as $W = \text{diag}(\frac{1}{1^TK}) K$ and the simplest denoising procedure consists of applying $W$ to the (vectorized) noisy image $y$, that is $\hat{z}=W y$. There are more sophisticated procedures such as twicing, where the filtering matrix is applied iteratively to the residual:<br />$$z^{k+1} = z^{k} + W(y-z^{k})$$ This procedure trades bias (over-smooth estimates) for variance (noisy estimates), and is stopped when a good balance is achieved. <strong>How does this relate to a convolutional neural network trained with a single image?</strong> It turns out that, as the network&#8217;s width increases, standard gradient descent optimization of the squared $\ell_2$ loss follows the twicing process, with a (fixed!) filter matrix $W=K$ where the<strong> pixel affinity function is available in closed form and only depends on the architecture of the network! </strong>For example, a simple single-hidden layer network with a filter of $k\times k$ pixels, corresponds to a non-local similarity function$$ k(y_i, y_j) = \frac{||y_i|| ||y_j||}{\pi} (\sin\phi+(\pi-\phi)\cos\phi)$$ where $\phi$ is the angle between patches $y_i$ and $y_j$ of $k\times k$ pixels each. Hence, we can compute the implicit filter in closed-form, without need to train a very large network!</p>
<p>Our analysis reveals that a neural network that, while the NTK theory accurately predicts the filter associated with networks trained using standard gradient descent, it falls short to explain the behavior of networks trained using the popular Adam optimizer. The latter achieves a larger change of weights in hidden layers, adapting the non-local filtering function during training. We evaluate our findings via extensive image denoising experiments. Please see the paper for more details!</p>


<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p></p>
</div></div>



<p></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Advances in single-photon lidar for autonomous vehicles</title>
		<link>https://tachella.github.io//2020/08/01/advances-in-single-photon-lidar-for-autonomous-vehicles/</link>
		
		<dc:creator><![CDATA[Julián]]></dc:creator>
		<pubDate>Sat, 01 Aug 2020 12:17:56 +0000</pubDate>
				<category><![CDATA[Lidar]]></category>
		<guid isPermaLink="false">https://tachella.github.io//?p=337</guid>

					<description><![CDATA[Authors: J. Rapp, J. Tachella, Y. Altmann, S. McLaughlin and V. K. Goyal Journal: IEEE Signal Processing Magazine Full paper: https://ieeexplore.ieee.org/abstract/document/9127841 We have put together a comprehensive survey of the latest signal processing techniques for single-photon lidar, covering Statistical modeling of single-photon waveforms (Poisson statistics, dead-time and discretization effects, etc.) 3D reconstruction algorithms (single depth&#46;&#46;&#46;]]></description>
										<content:encoded><![CDATA[<p><strong>Authors:</strong> J. Rapp, J. Tachella, Y. Altmann, S. McLaughlin and V. K. Goyal</p>
<p><strong>Journal:</strong> IEEE Signal Processing Magazine</p>
<p><strong>Full paper: </strong><a href="https://ieeexplore.ieee.org/abstract/document/9127841">https://ieeexplore.ieee.org/abstract/document/9127841</a></p>
<p>We have put together a <strong>comprehensive survey of the latest signal processing techniques for single-photon lidar</strong>, covering</p>
<ol>
<li>Statistical modeling of single-photon waveforms (Poisson statistics, dead-time and discretization effects, etc.)</li>
<li>3D reconstruction algorithms (single depth and multiple depth).</li>
</ol>
<p>These algorithms are explained in the <strong>context of autonomous vehicles, an emerging and very promising application of single-photon lidar technology</strong>. See the paper for more details! Code related to some of the algorithms discussed in the paper can be found <a href="https://tachella.github.io//code/">here</a> and <a href="https://ieeexplore.ieee.org/document/7932527/algorithms?tabFilter=code#algorithms">here</a>.</p>
<p><strong>Abstract: </strong>The safety and success of autonomous vehicles (AVs) depend on their ability to accurately map and respond to their surroundings in real time. One of the most promising recent technologies for depth mapping is single-photon lidar (SPL), which measures the time of flight of individual photons. The long-range capabilities (kilometers), excellent depth resolution (centimeters), and use of low-power (eye-safe) laser sources renders this modality a strong candidate for use in AVs. While presenting unique opportunities, the remarkable sensitivity of single-photon detectors introduces several signal processing challenges. The discrete nature of photon counting and the particular design of the detection devices means the acquired signals cannot be treated as arising in a linear system with additive Gaussian noise. Moreover, the number of useful photon detections may be small despite a large data volume, thus requiring careful modeling and algorithmic design for real-time performance. This article discusses the main working principles of SPL and summarizes recent advances in signal processing techniques for this modality, highlighting promising applications in AVs as well as a number of challenges for vehicular lidar that cannot be solved by better hardware alone.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Real-time 3D color imaging with single-photon lidar data</title>
		<link>https://tachella.github.io//2019/12/13/real-time-color-3d-reconstruction-from-single-photon-lidar-data/</link>
		
		<dc:creator><![CDATA[Julián]]></dc:creator>
		<pubDate>Fri, 13 Dec 2019 15:16:45 +0000</pubDate>
				<category><![CDATA[Lidar]]></category>
		<guid isPermaLink="false">https://tachella.github.io//?p=272</guid>

					<description><![CDATA[Authors: J. Tachella, Y. Altmann, S. McLaughlin and J-.Y. Tourneret Conference: IEEE CAMSAP 2019, Guadaloupe, West Indies Full paper:&#160;https://ieeexplore.ieee.org/document/9022496/ Online codes: gitlab.com/Tachella/real-time-single-photon-lidar Slides: CAMSAP19 from Julián Andrés Tachella Abstract: Single-photon lidar devices can acquire 3D data at very long range with high precision. Moreover, recent advances in lidar arrays have enabled acquisitions at very high&#46;&#46;&#46;]]></description>
										<content:encoded><![CDATA[<p><strong>Authors:</strong> J. Tachella, Y. Altmann, S. McLaughlin and J-.Y. Tourneret</p>
<p><strong>Conference:</strong> IEEE CAMSAP 2019, Guadaloupe, West Indies</p>
<p><strong>Full paper:&nbsp;</strong><a href="https://ieeexplore.ieee.org/document/9022496/">https://ieeexplore.ieee.org/document/9022496/</a></p>
<p><strong>Online codes: </strong><a href="https://gitlab.com/Tachella/real-time-single-photon-lidar">gitlab.com/Tachella/real-time-single-photon-lidar</a></p>
<p style="text-align: center;"><strong>Slides:</strong></p>
<p style="text-align: center;"><iframe loading="lazy" style="border: 1px solid #CCC; border-width: 1px; margin-bottom: 5px; max-width: 100%;" src="//www.slideshare.net/slideshow/embed_code/key/lliCs4XomO9fAA" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" allowfullscreen="allowfullscreen"> </iframe></p>
<div style="margin-bottom: 5px; text-align: center;"><strong> <a title="CAMSAP19" href="//www.slideshare.net/JulinAndrsTachella/camsap19-206850606" target="_blank" rel="noopener noreferrer">CAMSAP19</a> </strong> from <strong><a href="https://www.slideshare.net/JulinAndrsTachella" target="_blank" rel="noopener noreferrer">Julián Andrés Tachella</a></strong></div>
<h5>Abstract:</h5>
<p>Single-photon lidar devices can acquire 3D data at very long range with high precision. Moreover, recent advances in lidar<br />
arrays have enabled acquisitions at very high frame rates. However, these devices place a severe bottleneck on the reconstruction algorithms, which have to handle very large volumes of noisy data. Recently, real-time 3D reconstruction of distributed surfaces has been demonstrated obtaining information at one wavelength. Here, we propose a new algorithm that achieves color 3D reconstruction without increasing the execution time nor the acquisition process of the real-time single-wavelength reconstruction system. The algorithm uses a coded aperture that compresses the data by considering a subset of the wavelengths per pixel. The reconstruction algorithm is based on a plug-and-play denoising framework, which benefits from off-the-shelf point cloud and image denoisers. Experiments using real lidar data show the competitivity of the proposed method.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Real-time 3D reconstruction from single-photon lidar data using plug-and-play point cloud denoisers</title>
		<link>https://tachella.github.io//2019/10/29/real-time-3d-reconstruction-from-single-photon-lidar-data-using-plug-and-play-point-cloud-denoisers/</link>
		
		<dc:creator><![CDATA[Julián]]></dc:creator>
		<pubDate>Tue, 29 Oct 2019 11:42:10 +0000</pubDate>
				<category><![CDATA[Lidar]]></category>
		<guid isPermaLink="false">https://tachella.github.io//?p=210</guid>

					<description><![CDATA[Authors: J. Tachella, Y. Altmann, N. Mellado, R. Tobin, A. McCarthy, G. S. Buller, J-.Y. Tourneret and S. McLaughlin Journal: Nature Communications Full paper:  www.nature.com/articles/s41467-019-12943-7 Online codes: gitlab.com/Tachella/real-time-single-photon-lidar Abstract: Single-photon lidar has emerged as a prime candidate technology for depth imaging through challenging environments. Until now, a major limitation has been the significant amount of time&#46;&#46;&#46;]]></description>
										<content:encoded><![CDATA[<p><strong>Authors:</strong> J. Tachella, Y. Altmann, N. Mellado, R. Tobin, A. McCarthy, G. S. Buller, J-.Y. Tourneret and S. McLaughlin</p>
<p><strong>Journal:</strong> <em>Nature Communications</em></p>
<p><strong>Full paper:  </strong><a href="https://www.nature.com/articles/s41467-019-12943-7">www.nature.com/articles/s41467-019-12943-7</a></p>
<p><strong>Online codes: </strong><a href="https://gitlab.com/Tachella/real-time-single-photon-lidar">gitlab.com/Tachella/real-time-single-photon-lidar</a></p>
<div class="video-container"><iframe loading="lazy" title="Real-time 3D reconstruction of complex scenes using single-photon Lidar" width="740" height="416" src="https://www.youtube.com/embed/PzCcAoypUfM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
<h5>Abstract:</h5>
<p>Single-photon lidar has emerged as a prime candidate technology for depth imaging through challenging environments. Until now, a major limitation has been the significant amount of time required for the analysis of the recorded data. Here we show a new computational framework for real-time three-dimensional (3D) scene reconstruction from single-photon data. By combining statistical models with highly scalable computational tools from the computer graphics community, we demonstrate 3D reconstruction of complex outdoor scenes with processing times of the order of 20 ms, where the lidar data was acquired in broad daylight from distances up to 320 metres. The proposed method can handle an unknown number of surfaces in each pixel, allowing for target detection and imaging through cluttered scenes. This enables robust, real-time target reconstruction of complex moving scenes, paving the way for single-photon lidar at video rates for practical 3D imaging applications.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Bayesian 3D Reconstruction of subsampled multispectral lidar signals</title>
		<link>https://tachella.github.io//2019/10/01/multispectral-lidar/</link>
		
		<dc:creator><![CDATA[Julián]]></dc:creator>
		<pubDate>Tue, 01 Oct 2019 11:07:29 +0000</pubDate>
				<category><![CDATA[Lidar]]></category>
		<guid isPermaLink="false">https://tachella.github.io//?p=189</guid>

					<description><![CDATA[Authors: J. Tachella, Y. Altmann, M. Márquez, H. Arguello-Fuentes, J-.Y. Tourneret and S. McLaughlin Journal: IEEE Transactions on Computational Imaging Full paper: https://ieeexplore.ieee.org/document/8854866 Online codes: https://gitlab.com/Tachella/musapop Abstract: Light detection and ranging (lidar) single-photon devices capture range and intensity information from a 3D scene. This modality enables long range 3D reconstruction with high range precision and&#46;&#46;&#46;]]></description>
										<content:encoded><![CDATA[<p><strong>Authors:</strong> J. Tachella, Y. Altmann, M. Márquez, H. Arguello-Fuentes, J-.Y. Tourneret and S. McLaughlin</p>
<p><strong>Journal: </strong><em>IEEE Transactions on Computational Imaging</em></p>
<p><strong>Full paper:</strong> <a href="https://ieeexplore.ieee.org/document/8854866">https://ieeexplore.ieee.org/document/8854866</a></p>
<p><strong>Online codes: </strong><a href="https://gitlab.com/Tachella/musapop">https://gitlab.com/Tachella/musapop</a></p>
<h5>Abstract:</h5>
<p>Light detection and ranging (lidar) single-photon devices capture range and intensity information from a 3D scene. This modality enables long range 3D reconstruction with high range precision and low laser power. A multispectral single-photon lidar system provides additional spectral diversity, allowing the discrimination of different materials. However, the main drawback of such systems can be the long acquisition time needed to collect enough photons in each spectral band. In this work, we tackle this problem in two ways: first, we propose a Bayesian 3D reconstruction algorithm that is able to find multiple surfaces per pixel, using few photons, i.e., shorter acquisitions. In contrast to previous algorithms, the novel method processes the jointly all the spectral bands, obtaining better reconstructions using less photon detections. The proposed model promotes spatial correlation between neighbouring points within a given surface using spatial point processes. Secondly, we account for different spatial and spectral subsampling schemes, which reduce the total number of measurements, without significant degradation of the reconstruction performance. In this way, the total acquisition time, memory requirements and computational time can be significantly reduced. The experiments performed using both synthetic and real single-photon lidar data demonstrate the advantages of tailored sampling schemes over random alternatives. Furthermore, the proposed algorithm yields better estimates than other existing methods for multi-surface reconstruction using multispectral Lidar data.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Fast surface detection in single-photon lidar waveforms</title>
		<link>https://tachella.github.io//2019/09/05/fast-surface-detection-in-single-photon-lidar-waveforms/</link>
		
		<dc:creator><![CDATA[Julián]]></dc:creator>
		<pubDate>Thu, 05 Sep 2019 11:20:00 +0000</pubDate>
				<category><![CDATA[Lidar]]></category>
		<guid isPermaLink="false">https://tachella.github.io//?p=194</guid>

					<description><![CDATA[Authors: J. Tachella, Y. Altmann, M. Márquez, H. Arguello-Fuentes, J-.Y. Tourneret and S. McLaughlin Conference: European Signal Processing Conference (EUSIPCO), La Coruña, Spain (2019) Full paper: https://pureapps2.hw.ac.uk/ws/portalfiles/portal/25197816/PID5986663.pdf Extension: A multi-resolution version of this method was published here. Online codes: https://gitlab.com/Tachella/lidardetection Presentation slides: EUSIPCO19 from Julián Andrés Tachella Abstract: Single-photon light detection and ranging (lidar) devices can&#46;&#46;&#46;]]></description>
										<content:encoded><![CDATA[<p><strong>Authors</strong>: J. Tachella, Y. Altmann, M. Márquez, H. Arguello-Fuentes, J-.Y. Tourneret and S. McLaughlin</p>
<p><strong>Conference:</strong> European Signal Processing Conference (EUSIPCO), La Coruña, Spain (2019)</p>
<p><strong>Full paper:</strong> <a href="https://pureapps2.hw.ac.uk/ws/portalfiles/portal/25197816/PID5986663.pdf">https://pureapps2.hw.ac.uk/ws/portalfiles/portal/25197816/PID5986663.pdf</a></p>
<p><strong>Extension: </strong>A multi-resolution version of this method was published <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11138/111380T/On-fast-object-detection-using-single-photon-lidar-data/10.1117/12.2527685.short?SSO=1">here</a>.</p>
<p><strong>Online codes: </strong><a href="https://gitlab.com/Tachella/lidardetection">https://gitlab.com/Tachella/lidardetection</a></p>
<p><strong>Presentation slides:</strong></p>
<div class="video-container"><iframe loading="lazy" title="EUSIPCO19" src="https://www.slideshare.net/slideshow/embed_code/key/huzswkYl9Texnf" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> </p>
<div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/JulinAndrsTachella/eusipco19" title="EUSIPCO19" target="_blank">EUSIPCO19</a> </strong> from <strong><a href="https://www.slideshare.net/JulinAndrsTachella" target="_blank">Julián Andrés Tachella</a></strong> </div>
</div>
<h5>Abstract:</h5>
<p>Single-photon light detection and ranging (lidar) devices can be used to obtain range and reflectivity information from 3D scenes. However, reconstructing the 3D surfaces from the raw waveforms can be very challenging, in particular when the number of spurious background detections is large compared to the number of signal detections. In this paper, we introduce a new and fast detection algorithm, which can be used to assess the presence of objects/surfaces in each waveform, allowing only the histograms where the imaged surfaces are present to be further processed. The method is compared to state-of-the-art 3D reconstruction methods using synthetic and real single-photon data and the results illustrate its benefits for fast and robust target detection using single-photon data.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>3D Reconstruction using single-photon lidar data: Exploiting the widths of the returns</title>
		<link>https://tachella.github.io//2019/05/05/estimating-the-widths-of-the-returns/</link>
		
		<dc:creator><![CDATA[Julián]]></dc:creator>
		<pubDate>Sun, 05 May 2019 10:43:53 +0000</pubDate>
				<category><![CDATA[Lidar]]></category>
		<guid isPermaLink="false">https://tachella.github.io//?p=181</guid>

					<description><![CDATA[Authors: J. Tachella, Y. Altmann, J-.Y. Tourneret and S. McLaughlin Conference: International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, 2019. Full-text: https://ieeexplore.ieee.org/abstract/document/8683075 Short presentation: ICASSP19 from Julián Andrés Tachella Abstract: Single-photon light detection and ranging (lidar) data can be used to capture depth and intensity profiles of a 3D scene. In a general setting, the&#46;&#46;&#46;]]></description>
										<content:encoded><![CDATA[<h5>Authors:</h5>
<p>J. Tachella, Y. Altmann, J-.Y. Tourneret and S. McLaughlin</p>
<p><strong>Conference:</strong> International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, 2019.</p>
<p><strong>Full-text:</strong> <a href="https://ieeexplore.ieee.org/abstract/document/8683075">https://ieeexplore.ieee.org/abstract/document/8683075</a></p>
<h5>Short presentation:</h5>
<div class="video-container"><iframe loading="lazy" title="ICASSP19" src="https://www.slideshare.net/slideshow/embed_code/key/8nGrJtfx73XrqO" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> </p>
<div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/JulinAndrsTachella/icassp19" title="ICASSP19" target="_blank">ICASSP19</a> </strong> from <strong><a href="https://www.slideshare.net/JulinAndrsTachella" target="_blank">Julián Andrés Tachella</a></strong> </div>
</div>
<h5>Abstract:</h5>
<p>Single-photon light detection and ranging (lidar) data can be used to capture depth and intensity profiles of a 3D scene. In a general setting, the scenes can have an unknown number of surfaces per pixel (semi-transparent surfaces or outdoor measurements), high background noise (strong ambient illumination), can be acquired by systems with a broad instrumental response (non-parallel laser beam with respect to the target surface) and with possibly high attenuating media (underwater conditions). The existing methods generally tackle only a subset of these problems and can fail in a more general scenario. In this paper, we propose a new 3D reconstruction algorithm that can handle all the aforementioned difficulties. The novel algorithm estimates the broadening of the impulse response, considers the attenuation induced by scattering media, while allowing for multiple surfaces per pixel. A series of experiments performed in real long-range and underwater lidar datasets demonstrate the performance of the proposed method.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
